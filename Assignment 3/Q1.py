# -*- coding: utf-8 -*-
"""RNN_SK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ipu_0aJJmMflKrAS13FLp10jbqPxDCBO
"""

from google.colab import drive
drive.mount('/content/drive/')

import tarfile
my_tar = tarfile.open('/content/drive/MyDrive/dakshina_dataset_v1.0.tar')
my_tar.extractall('./RNN folder') # specify which folder to extract to
my_tar.close()

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

from keras.models import Model
from keras.layers import Input, LSTM, Dense, RNN, GRU, SimpleRNN

train = pd.read_csv("/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv",delimiter="\t",header=None,names = ['hindi', 'word', 'number'])
val = pd.read_csv('/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv',delimiter="\t",header=None,names = ['hindi', 'word', 'number'])
test = pd.read_csv('/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv',delimiter="\t",header=None,names = ['hindi', 'word', 'number'])

train.dropna(inplace = True)
df = pd.concat([train, val, test])
df['word'] = df['word'].apply(lambda x: x + "\n")
df['hindi'] = df['hindi'].apply(lambda x: "\t" + x + "\n")

def get_unique_char(dataset):
  my_set = set()
  for i in dataset:
    for char in i:
      my_set.add(char)
  return my_set

def get_req_vec(tr_word, tr_hindi):
  tr_input_char = get_unique_char(tr_word)
  tr_target_char = get_unique_char(tr_hindi)
  num_encoder_tokens = len(tr_input_char)
  num_decoder_tokens = len(tr_target_char)
  max_encoder_seq_length = max([len(txt) for txt in tr_word])
  max_decoder_seq_length = max([len(txt) for txt in tr_hindi])
  return [num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length]

def generate_dataset(input_texts, target_texts, params):
  input_characters = sorted(list(get_unique_char(input_texts)))
  target_characters = sorted(list(get_unique_char(target_texts)))

  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])
  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])

  num_encoder_tokens = params[0]
  num_decoder_tokens = params[1]
  max_encoder_seq_length = params[2] 
  max_decoder_seq_length = params[3]

  encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype="float32"
  )
  decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32"
  )
  decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32"
  )

  for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.0
    encoder_input_data[i, t + 1 :, input_token_index["\n"]] = 1.0
    for t, char in enumerate(target_text):
        decoder_input_data[i, t, target_token_index[char]] = 1.0
        if t > 0:
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0
    decoder_input_data[i, t + 1 :, target_token_index["\n"]] = 1.0
    decoder_target_data[i, t:, target_token_index["\n"]] = 1.0

  return [encoder_input_data, decoder_input_data], decoder_target_data

params = get_req_vec(list(df.word), list(df.hindi))
data_X, data_Y = generate_dataset(list(df.word), list(df.hindi), params)

train_X = [data_X[0][:train.shape[0]], data_X[1][:train.shape[0]]]
train_Y = data_Y[:train.shape[0]]
val_X = [data_X[0][train.shape[0]: train.shape[0] + val.shape[0]], data_X[1][train.shape[0]: train.shape[0] + val.shape[0]]]
val_Y = data_Y[train.shape[0]: train.shape[0] + val.shape[0]]

test_X = [data_X[0][train.shape[0] + val.shape[0]:], data_X[1][train.shape[0] + val.shape[0]:]]
test_Y = data_Y[train.shape[0] + val.shape[0]:]

latent_dim = 256
batch_size = 64
epochs = 5

num_encoder_tokens = params[0]
num_decoder_tokens = params[1]
max_encoder_seq_length = params[2] 
max_decoder_seq_length = params[3]

def generate_latent_dim(hidden_layer_size, num_encode_layers):
  if num_encode_layers == 1: return [hidden_layer_size]
  elif num_encode_layers == 2:
    if hidden_layer_size == 16:
      return [16, 32]
    else:
      return [hidden_layer_size, hidden_layer_size // 2]
  else:
    if hidden_layer_size < 64:
      return [hidden_layer_size, hidden_layer_size * 2, hidden_layer_size * 4]
    else:
      return [hidden_layer_size, hidden_layer_size // 2, hidden_layer_size // 4]

def build_seq2seq_model(num_encode_layers, num_decode_layers, hidden_layer_size, dropout, beam_size, cell_type = 'LSTM'):
  model = keras.Sequential()
  latent_dims = generate_latent_dim(hidden_layer_size, num_encode_layers)

  encoder_inputs = Input(shape=(None, num_encoder_tokens))

  outputs = encoder_inputs
  encoder_states = []
  for j in range(len(latent_dims))[::-1]:
    if cell_type == 'LSTM':
      outputs, h, c = LSTM(latent_dims[j], return_state = True, return_sequences = True, dropout = dropout)(outputs)
      encoder_states += [h, c]
    
    elif cell_type == 'GRU':
      outputs, h = GRU(latent_dims[j], return_state = True, return_sequences = True, dropout = dropout)(outputs)
      encoder_states += [h]
    elif cell_type == 'RNN':
      outputs, h = SimpleRNN(latent_dims[j], return_state = True, return_sequences = True, dropout = dropout)(outputs)
      encoder_states += [h]

  decoder_inputs = Input(shape=(None, num_decoder_tokens))

  outputs = decoder_inputs
  output_layers = []
  for j in range(len(latent_dims)):
    if cell_type == 'LSTM':
      output_layers.append(
          LSTM(latent_dims[len(latent_dims) - j - 1], return_sequences=True, return_state=True, dropout = dropout)
      )
      outputs, dh, dc = output_layers[-1](outputs, initial_state=encoder_states[2*j : 2*(j + 1)])

    elif cell_type == 'GRU':
      output_layers.append(
          GRU(latent_dims[len(latent_dims) - j - 1], return_sequences=True, return_state=True, dropout = dropout)
      )
      outputs, dh = output_layers[-1](outputs, initial_state=encoder_states[j])

    elif cell_type == 'RNN':
      output_layers.append(
          SimpleRNN(latent_dims[len(latent_dims) - j - 1], return_sequences=True, return_state=True, dropout = dropout)
      )
      outputs, dh = output_layers[-1](outputs, initial_state=encoder_states[j])

  decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')
  decoder_outputs = decoder_dense(outputs)
  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

  return model

model = build_seq2seq_model(1, 3, 1024, 0, 0, 'GRU')
model.summary()

model.compile(
    optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"]
)
model.fit(
    train_X,
    train_Y,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (val_X, val_Y),
)

