# -*- coding: utf-8 -*-
"""RNN_SK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ipu_0aJJmMflKrAS13FLp10jbqPxDCBO
"""

from google.colab import drive
drive.mount('/content/drive/')

import tarfile
my_tar = tarfile.open('/content/drive/MyDrive/dakshina_dataset_v1.0.tar')
my_tar.extractall('./RNN folder') # specify which folder to extract to
my_tar.close()

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

train = pd.read_csv("/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv",delimiter="\t",header=None,names = ['hindi', 'word', 'number'])
val = pd.read_csv('/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv',delimiter="\t",header=None,names = ['hindi', 'word', 'number'])
test = pd.read_csv('/content/drive/MyDrive/RNN folder/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv',delimiter="\t",header=None,names = ['hindi', 'word', 'number'])

train.head()
val.head()
test.head()

train.dropna(inplace = True)

def get_unique_char(dataset):
  my_set = set()
  for i in dataset:
    for char in i:
      my_set.add(char)
  return my_set

def get_req_vec(tr_word, tr_hindi, val_word, val_hindi):
  tr_input_char = get_unique_char(tr_word)
  tr_target_char = get_unique_char(tr_hindi)
  val_input_char = get_unique_char(val_word)
  val_target_char = get_unique_char(val_hindi)
  num_encoder_tokens = len(tr_input_char.union(val_input_char))
  num_decoder_tokens = len(tr_target_char.union(val_target_char))
  max_encoder_seq_length = max([len(txt) for txt in tr_word] + [len(txt) for txt in val_word])
  max_decoder_seq_length = max([len(txt) for txt in tr_hindi] + [len(txt) for txt in val_hindi])
  return [num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length]

def generate_dataset(input_texts, target_texts, params):
  input_characters = sorted(list(get_unique_char(input_texts)))
  target_characters = sorted(list(get_unique_char(target_texts)))

  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])
  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])

  num_encoder_tokens = params[0]
  num_decoder_tokens = params[1]
  max_encoder_seq_length = params[2] 
  max_decoder_seq_length = params[3]

  encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype="float32"
  )
  decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32"
  )
  decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32"
  )

  for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
      encoder_input_data[i, t, input_token_index[char]] = 1.0
      
    for t, char in enumerate(target_text):
      # decoder_target_data is ahead of decoder_input_data by one timestep
      decoder_input_data[i, t, target_token_index[char]] = 1.0
      if t > 0:
          # decoder_target_data will be ahead by one timestep
          # and will not include the start character.
          decoder_target_data[i, t - 1, target_token_index[char]] = 1.0

  return [encoder_input_data, decoder_input_data], decoder_target_data

params = get_req_vec(list(train.word), list(train.hindi), list(val.word), list(val.hindi))
train_X, train_Y = generate_dataset(list(train.word), list(train.hindi), params)
val_X, val_Y = generate_dataset(list(val.word), list(val.hindi), params)

latent_dim = 256
batch_size = 64
epochs = 5

num_encoder_tokens = params[0]
num_decoder_tokens = params[1]
max_encoder_seq_length = params[2] 
max_decoder_seq_length = params[3]

# Define an input sequence and process it.
encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))
encoder = keras.layers.LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)

# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))

# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences = True, return_state=True)
decoder_outputs, _ , _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)
decoder_dense = keras.layers.Dense(num_decoder_tokens, activation="softmax")
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(
    optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"]
)
model.fit(
    train_X,
    train_Y,
    batch_size = batch_size,
    epochs = epochs,
    validation_data = (val_X, val_Y),
)

