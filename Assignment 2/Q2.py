# -*- coding: utf-8 -*-
"""DL_Assignment_Q2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZMA0SV95lZqRRxx6cHvcJH-cwz1s8bFQ
"""

from google.colab import drive
drive.mount("/content/gdrive")

! pip install wandb

!nvidia-smi

import keras
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Dropout, Activation, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from keras.losses import CategoricalCrossentropy
import PIL
import wandb
from wandb.keras import WandbCallback
from keras.callbacks import EarlyStopping
from keras import optimizers

from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.applications.resnet50 import ResNet50
from keras.applications.inception_v3 import InceptionV3
from keras.applications.xception import Xception
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.optimizers import SGD, Adam, RMSprop

wandb.login()

IMAGE_SIZE = [224, 224]
inception = VGG16(input_shape = IMAGE_SIZE + [3], weights='imagenet', include_top=False)
c = 0
for layer in inception.layers:
  c += 1
  layer.trainable = False

print(c)



def train():
  default_hyperparams = dict(
      tr_model = 'VGG16',
      learning_rate=0.01,
      optimizer = 'Adam',
      freeze = 0.5,
      epochs = 5,
      img_size = 224,
      data_aug = 0,
      batch_size = 32,
  )    
    
  wandb.init(config = default_hyperparams)
  config = wandb.config

  train_data_dir= '/content/gdrive/MyDrive/inaturalist_12K/train'

  batch_size = config.batch_size

  if config.data_aug:      
    train_datagen = ImageDataGenerator(
                  rescale=1./255,
                  rotation_range=45, 
                  width_shift_range=.15, 
                  height_shift_range=.15, 
                  horizontal_flip=True, 
                  zoom_range=0.5,
                  validation_split=0.1) 

  else:      
    train_datagen = ImageDataGenerator(
                  rescale=1./255,
                  validation_split=0.1) # set validation split

  train_it = train_datagen.flow_from_directory(
      train_data_dir,
      target_size=(224,224),
      batch_size=batch_size,
      class_mode='categorical',
      subset='training') # set as training data

  val_it = train_datagen.flow_from_directory(
      train_data_dir, # same directory as training data
      target_size=(224,224),
      batch_size=batch_size,
      class_mode='categorical',
      subset='validation') # set as validation data

  IMAGE_SIZE = [config.img_size, config.img_size]

  layers_in_model = 14 #default

  if config.tr_model == 'VGG16':
    trained_model = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
    layers_in_model = 14

  elif config.tr_model == 'InceptionV3':
    trained_model = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
    layers_in_model = 14
  
  elif config.tr_model == 'ResNet50':
    trained_model = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
    layers_in_model = 14

  elif config.tr_model == 'Xception':
    trained_model = Xception(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
    layers_in_model = 20

  elif config.tr_model == 'InceptionResNetV2':
    trained_model = InceptionResNetV2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
    layers_in_model = 20

  if config.freeze == -1:
    for layer in trained_model.layers:
      layer.trainable = False
  
  else:
    for layer in trained_model.layers[:config.freeze]:
      layer.trainable = False

    for layer in trained_model.layers[config.freeze:]:
      layer.trainable = True

  x = Flatten()(trained_model.output)
  prediction = Dense(10, activation='softmax')(x)
  model = Model(inputs=trained_model.input, outputs=prediction)

  if config.optimizer == 'Adam':
    if config.learning_rate == 0:
      model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
      )  
    
    else:
      model.compile(optimizer=Adam(lr = config.learning_rate, decay = 1e-6), loss='categorical_crossentropy', metrics=['accuracy'])

  if config.optimizer == 'rmsprop':
    if config.learning_rate == 0:
      model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        metrics=['accuracy']
      )  
    
    else:
      model.compile(optimizer=RMSprop(lr = config.learning_rate, decay = 1e-6), loss='categorical_crossentropy', metrics=['accuracy'])

  if config.optimizer == 'sgd':
    if config.learning_rate == 0:
      model.compile(
        loss='categorical_crossentropy',
        optimizer='sgd',
        metrics=['accuracy']
      )  
    
    else:
      model.compile(optimizer=SGD(lr = config.learning_rate, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])

  model.fit(
    train_it,
    steps_per_epoch = train_it.samples //batch_size,
    validation_data = val_it, 
    validation_steps = val_it.samples // batch_size,
    epochs = config.epochs,
    callbacks=[WandbCallback(data_type='image',validation_data = val_it,verbose=1),EarlyStopping(patience=10,restore_best_weights=True)])
  #train_loss,train_accuracy = model.evaluate(train_it, callbacks=[WandbCallback()])
  #val_loss, val_accuracy = model.evaluate(val_it, callbacks=[WandbCallback()])
  #wandb.log({'val_loss':val_loss,'val_accuracy':val_accuracy*100,'train_loss': train_loss,'train_accuracy':train_accuracy*100 }) # wandb.log to track custom metrics

